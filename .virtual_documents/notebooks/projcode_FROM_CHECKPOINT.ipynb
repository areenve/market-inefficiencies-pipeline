from google.colab import files
up = files.upload()  # select cross_venue_dislocations.zip from your computer
!unzip -o cross_venue_dislocations.zip -d .
%cd cross_venue_dislocations
!ls -la


#install dependencies
!python -V
!pip install -r requirements.txt --quiet


# stop any old one (safe if none)
!pkill -f run_collect.py || true

# start and log output to a file
!nohup python -u run_collect.py > data/collector.out 2>&1 & echo "collector started"


!python run_detect.py
!python run_backtest.py


import yaml
cfg = yaml.safe_load(open("config.yaml"))
# example: lower costs and persistence for more events
cfg["costs_bps"].update({"fee_bps":2,"half_spread_bps":1,"slippage_bps":3})
cfg["persistence_ms"] = 300
yaml.dump(cfg, open("config.yaml","w"), sort_keys=False)
print("Threshold (bps):", cfg["costs_bps"]["fee_bps"]+cfg["costs_bps"]["half_spread_bps"]+cfg["costs_bps"]["slippage_bps"]+cfg["costs_bps"].get("extra_bps",0))


!python run_detect.py
!python run_backtest.py


# CSV of all ticks (appendix)
import sqlite3, pandas as pd
conn = sqlite3.connect('data/dislocations.sqlite3')
pd.read_sql_query('select * from ticks order by ts_ms', conn).to_csv('data/ticks_export.csv', index=False)
conn.close()


# Figures: time series with event shading + histogram
import matplotlib.pyplot as plt
from core.detector import run_detection
res = run_detection(lookback_ms=60*60*1000)
m, e, thr = res['metrics'], res['events'], res['threshold_bps']

plt.figure(figsize=(10,4)); m['spread_bps'].plot(); plt.axhline(thr, ls='--')
for _, r in e.iterrows(): plt.axvspan(r['start_ms'], r['end_ms'], alpha=0.15)
plt.title("Spread (bps) with threshold & events"); plt.xlabel("timestamp (ms)"); plt.ylabel("bps")
plt.tight_layout(); plt.savefig("figure_spread_timeseries.png", dpi=300)

plt.figure(figsize=(6,4)); m['spread_bps'].dropna().hist(bins=50); plt.axvline(thr, ls='--')
plt.title("Distribution of spread (bps)"); plt.xlabel("bps"); plt.ylabel("count")
plt.tight_layout(); plt.savefig("figure_spread_hist.png", dpi=300)

print("Saved: figure_spread_timeseries.png, figure_spread_hist.png")


from google.colab import files
for fn in ["data/dislocations.sqlite3","data/ticks_export.csv",
           "figure_spread_timeseries.png","figure_spread_hist.png"]:
    try: files.download(fn)
    except: pass


import os, sqlite3, pandas as pd, yaml

cfg = yaml.safe_load(open("config.yaml"))
db = cfg.get("db_path", "data/dislocations.sqlite3")
print("DB path:", db, "| exists:", os.path.exists(db))

if os.path.exists(db):
    conn = sqlite3.connect(db)
    # how many ticks total?
    print(pd.read_sql_query("SELECT COUNT(*) AS rows FROM ticks", conn))
    # show a few recent rows
    display(pd.read_sql_query("SELECT * FROM ticks ORDER BY ts_ms DESC LIMIT 10", conn))
    conn.close()


import sqlite3, pandas as pd
conn = sqlite3.connect("data/dislocations.sqlite3")

# per-venue latest quotes
q1 = """
WITH latest AS (
  SELECT venue, MAX(ts_ms) AS ts
  FROM ticks GROUP BY venue
)
SELECT t.venue, t.ts_ms, t.bid, t.ask, t.mid
FROM ticks t JOIN latest l
  ON t.venue = l.venue AND t.ts_ms = l.ts
ORDER BY t.venue;
"""
display(pd.read_sql_query(q1, conn))

# basic time window stats (last 60 minutes)
q2 = """
SELECT
  MIN(ts_ms) AS start_ms,
  MAX(ts_ms) AS end_ms,
  COUNT(*)  AS n_ticks
FROM ticks
WHERE ts_ms >= (SELECT MAX(ts_ms) - 60*60*1000 FROM ticks);
"""
display(pd.read_sql_query(q2, conn))

conn.close()


import sqlite3, pandas as pd
conn = sqlite3.connect("data/dislocations.sqlite3")
pd.read_sql_query("SELECT * FROM ticks ORDER BY ts_ms", conn).to_csv("data/ticks_export.csv", index=False)
conn.close()
print("Saved: data/ticks_export.csv")



from core.detector import run_detection
import pandas as pd

res = run_detection(lookback_ms=60*60*1000)  # last 60 min
events = res['events'].copy()
if not events.empty:
    # keep the most informative columns
    keep = ["start_ms","end_ms","duration_ms","max_mid","min_mid","spread_bps"]
    events[keep].to_csv("data/events_last60min.csv", index=False)
    display(events[keep].head(8))
    print("Saved: data/events_last60min.csv")
else:
    print("No events in the last 60 minutes.")


import numpy as np, pandas as pd
from core.detector import run_detection

# Analyze last 60 minutes (change if you want)
LOOKBACK_MS = 60*60*1000
res = run_detection(lookback_ms=LOOKBACK_MS)

m  = res['metrics']          # time series with 'spread_bps'
ev = res['events'].copy()    # events table
thr = res['threshold_bps']

print("Available event columns:", ev.columns.tolist())

if ev.empty:
    print("No events in the last 60 minutes.")
else:
    # If 'peak_bps' not provided, compute it from the metrics segment
    if 'peak_bps' not in ev.columns:
        # ensure metrics uses a numeric index of timestamps
        ts = m.index if isinstance(m.index, pd.Index) else pd.Index(m.index)
        def peak_bps(row):
            seg = m[(ts >= row['start_ms']) & (ts <= row['end_ms'])]
            return float(seg['spread_bps'].max()) if not seg.empty else np.nan
        ev['peak_bps'] = ev.apply(peak_bps, axis=1)

    # Pick columns that actually exist
    preferred = ["start_ms","end_ms","duration_ms","peak_bps","max_mid","min_mid"]
    keep = [c for c in preferred if c in ev.columns]

    # Save a compact table
    out = ev[keep].sort_values("start_ms")
    display(out.head(10))
    out.to_csv("data/events_last60min.csv", index=False)
    print("Saved: data/events_last60min.csv")



from google.colab import files
files.download("data/events_last60min.csv")


import pandas as pd, datetime as dt
ev = pd.read_csv("data/events_last60min.csv")
def to_utc(ms):
    return dt.datetime.utcfromtimestamp(ms/1000).strftime("%Y-%m-%d %H:%M:%S")
if "start_ms" in ev and "end_ms" in ev:
    ev["start_utc"] = ev["start_ms"].apply(to_utc)
    ev["end_utc"]   = ev["end_ms"].apply(to_utc)
    if "duration_ms" in ev:
        ev["duration_s"] = (ev["duration_ms"]/1000).round(2)
cols = [c for c in ["start_utc","end_utc","duration_s","peak_bps"] if c in ev.columns]
pretty = ev[cols].head(8)  # top 8 rows for the poster
pretty.to_csv("data/events_pretty.csv", index=False)
pretty


from google.colab import files
files.download("data/events_pretty.csv")


from google.colab import files
for fn in ["results_box.png","results_box.txt","results_box.json"]:
    try: files.download(fn)
    except: pass


from google.colab import files
for fn in [
    "data/events_pretty.csv",        # readable event table (UTC, duration, peak_bps)
    "data/events_last60min.csv",     # raw event slice
    "results_box.png", "results_box.txt", "results_box.json",
    "figure_spread_timeseries.png", "figure_spread_hist.png",
    "data/ticks_export.csv",         # full tick dump (if you created it)
    "data/dislocations.sqlite3"      # the SQLite database
]:
    try:
        files.download(fn)
    except Exception as e:
        print("Skip:", fn, "-", e)


from google.colab import drive
drive.mount('/content/drive')
!mkdir -p "/content/drive/MyDrive/cvd_results"
!cp -f data/dislocations.sqlite3 data/events_pretty.csv data/events_last60min.csv \
      figure_spread_timeseries.png figure_spread_hist.png results_box.png \
      "/content/drive/MyDrive/cvd_results/" 2>/dev/null || true
!ls -la "/content/drive/MyDrive/cvd_results"



!pkill -f run_collect.py

